!pip install pyspark

from pyspark import SparkContext

# âœ… Safe initialization
sc = SparkContext.getOrCreate()

# a) Load dataset into RDD
data = [
    (1, "Alice", "HR", 50000, 2012),
    (2, "Bob", "IT", 60000, 2016),
    (3, "Charlie", "Finance", 55000, 2014),
    (4, "David", "IT", 65000, 2013)
]
rdd = sc.parallelize(data)

# b) Count records
print("Total Employees:", rdd.count())

# c) Filter employees who joined after 2014
filtered = rdd.filter(lambda x: x[4] <= 2014)

# d) Map to employee names
names = filtered.map(lambda x: x[1])
print("Employees joined on/before 2014:", names.collect())

# e) Average salary per department
dept_salary = rdd.map(lambda x: (x[2], (x[3], 1))) \
                 .reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1])) \
                 .mapValues(lambda x: x[0]/x[1])
print("Average Salary by Department:", dept_salary.collect())

sc.stop()
