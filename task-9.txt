from pyspark import SparkContext
sc = SparkContext("local", "EmployeeRDD")

# a) Load the Dataset into an RDD
rdd = sc.textFile("employees.csv")

# Skip header (if your CSV has column names)
header = rdd.first()
data = rdd.filter(lambda x: x != header)

# Split each line by comma
data = data.map(lambda x: x.split(","))

# b) Count the number of records
print("Total Records:", data.count())

# c) Filter employees who joined after 2014
filtered = data.filter(lambda x: int(x[4]) > 2014)
print("Employees joined after 2014:", filtered.collect())

# d) Map to get only employee names
names = data.map(lambda x: x[1])
print("Employee Names:", names.collect())

# e) Average salary per department
dept_avg = data.map(lambda x: (x[2], (float(x[3]), 1))) \
               .reduceByKey(lambda a,b:(a[0]+b[0], a[1]+b[1])) \
               .mapValues(lambda x: x[0]/x[1])
print("Average Salary by Department:", dept_avg.collect())

sc.stop()
